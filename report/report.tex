%% USEFUL LINKS:
%% -------------
%%
%% - UiO LaTeX guides:          https://www.mn.uio.no/ifi/tjenester/it/hjelp/latex/
%% - Mathematics:               https://en.wikibooks.org/wiki/LaTeX/Mathematics
%% - Physics:                   https://ctan.uib.no/macros/latex/contrib/physics/physics.pdf
%% - Basics of Tikz:            https://en.wikibooks.org/wiki/LaTeX/PGF/Tikz
%% - All the colors!            https://en.wikibooks.org/wiki/LaTeX/Colors
%% - How to make tables:        https://en.wikibooks.org/wiki/LaTeX/Tables
%% - Code listing styles:       https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
%% - \includegraphics           https://en.wikibooks.org/wiki/LaTeX/Importing_Graphics
%% - Learn more about figures:  https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions
%% - Automagic bibliography:    https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management  (this one is kinda difficult the first time)
%%
%%                              (This document is of class "revtex4-1", the REVTeX Guide explains how the class works)
%%   REVTeX Guide:              http://www.physics.csbsju.edu/370/papers/Journal_Style_Manuals/auguide4-1.pdf
%%
%% COMPILING THE .pdf FILE IN THE LINUX IN THE TERMINAL
%% ----------------------------------------------------
%%
%% [terminal]$ pdflatex report_example.tex
%%
%% Run the command twice, always.
%%
%% When using references, footnotes, etc. you should run the following chain of commands:
%%
%% [terminal]$ pdflatex report_example.tex
%% [terminal]$ bibtex report_example
%% [terminal]$ pdflatex report_example.tex
%% [terminal]$ pdflatex report_example.tex
%%
%% This series of commands can of course be gathered into a single-line command:
%% [terminal]$ pdflatex report_example.tex && bibtex report_example.aux && pdflatex report_example.tex && pdflatex report_example.tex
%%
%% ----------------------------------------------------


\documentclass[english,notitlepage,reprint,nofootinbib]{revtex4-2}  % defines the basic parameters of the document
% For preview: skriv i terminal: latexmk -pdf -pvc filnavn
% If you want a single-column, remove "reprint"

% Allows special characters (including æøå)
\usepackage[utf8]{inputenc}
% \usepackage[english]{babel}

%% Note that you may need to download some of these packages manually, it depends on your setup.
%% I recommend downloading TeXMaker, because it includes a large library of the most common packages.

\usepackage{physics,amssymb}  % mathematical symbols (physics imports amsmath)
\usepackage{amsmath}
\usepackage{graphicx}         % include graphics such as plots
\usepackage{xcolor}           % set colors
\usepackage{hyperref}         % automagic cross-referencing
\usepackage{listings}         % display code
\usepackage{subfigure}        % imports a lot of cool and useful figure commands
% \usepackage{float}
%\usepackage[section]{placeins}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{quantikz}
% defines the color of hyperref objects
% Blending two colors:  blue!80!black  =  80% blue and 20% black
\hypersetup{ % this is just my personal choice, feel free to change things
	colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}}


% ===========================================

%\addbibresource{refs.bib} % Entries are in the "refs.bib" file

\begin{document}
	
	\title{Simulation of the 2D Ising model with Monte Carlo Markov Chain}  % self-explanatory
	\author{} % self-explanatory
	\date{\today}                             % self-explanatory
	\noaffiliation                            % ignore this, but keep it.
	
	%This is how we create an abstract section.
	\begin{abstract}
We study the 2D ferromagnetic Ising Model with the Monte Carlo Markov chain. We
use the Metropolis algorithm to sample the probability distribution of the Ising
model.
	As proven in 1944 by Lars Onsager\cite{PhysRev.65.117}, we have a phase
transition at the critical temperature $T_c(L=\infty)=2.269 \mathrm{J / k}$. We
find a matching numerical critical temperature
$T_c(L=\infty) = 2.29 \pm 0.04 \mathrm{J / k}$. To verify that the Metropolis
algorithm is working correctly, we used the analytical solution in the case  of
2x2 lattice. Additionally, we look at the distribution of our energy normalized
by spin $\epsilon$ for a 20x20 lattice with different temperatures. Temperature plays an important role in the shape of our distribution. Since
we can only compute theoretical values when we reach equilibrium, we used the
"Burn-in"  method to determine the number of cycles needed to reach equilibrium.
For a 20x20 lattice, we determine that around 10000 cycles are enough to reach
equilibrium.
\end{abstract}
	\maketitle	
	
	
	% ===========================================
	\section{Introduction} \label{sec:introduction}
	%
	The Ising model is a statistical mechanics model that describes ferromagnetism by a system of spins on a lattice.
	Ferromagnetism is a property of certain materials, allowing them to form a permanent magnet. A phase transition is one important concept in the Ising model for dimensions
	higher than 1. A phase transition is an extreme change in the physical properties. In our case, the model will go high magnetization to close to
	0 with increasing temperature. $\colorbox{red}{More detail about Ising model? Other use in science perhaps}$\\

	There is no analytical solution for the Ising model in dimensions $>$ 2.
However, we can use the Monte Carlo Markov Chain (MCMC) algorithm to simulate
our model.
	MCMC was developed in the 40s by Stanislaw Ulam and John Von Neumann in Los
Alamos. So the original paper could not be found, but the history is well
documented in e.g.\cite{Robert_2011}. The basic idea is to use randomness to
solve a deterministic problem and has shown remarkable
	results in many situations $\colorbox{red}{some example?}$ in computational
physics. In this
	paper, we will apply the Monte Carlo methods to solve the 2D Ising Model, and
we will use the
	Metropolis algorithm to sample the distribution. As explained in more
details in section
	\ref{sec:theory}, we will use the relation between finite lattice and
infinite lattice to
	estimate the critical temperature, $T_c(L=\infty)$ solved analytically by
Lars Onsager in 1944. \\

	In section \ref{sec:theory}, we will formally explain the Ising Model
	with the analytical solution for the 2x2 case used as a benchmark for our algorithm. In the
	next section \ref{sec:methods} we will discuss the numerical methods and the algorithms
	used. Section \ref{sec:results} and section \ref{sec:discussion} respectively presents our
	results from our simulation and an analysis of them. Finally, section \ref{sec:conclusion} summarises this paper's main points.
	\section{Theory}\label{sec:theory}
\subsection{2D Ising Model} \label{subsec:Ising}
	The 2D Ising model is a 2-dimensional lattice of equal length and row (L) composed of spins
	$s_i$. So a lattice of size L will wield $\rm LxL=N$ spins. Each spin can take one of the
	following values $\{-1,	1\}$ representing their magnetic dipole. Every spin can interact
	with its next neighbour, meaning that every spin can have up to four interactions. One
	important detail in physics is what to do with the boundaries. In our case, we will consider
	a periodic boundary condition. This means that we will always consider four interactions
	for each spin. This is the geometrical equivalent of having a lattice in the shape of a
	torus. We will denote the spin configuration by $\textbf{s}=\{s_1, s_2, ..., s_N\}$. Which allows us
	to write the Hamiltonian

	\begin{equation}
		H(\textbf{s}) = -J \sum_{\langle kl \rangle} s_ks_l + h \sum_{i=1}^{N} s_i \label{eq:hamilton}
	\end{equation}

	Eq. \ref{eq:hamilton} is composed of two terms. The first one is the contribution of every
	neighbour. $\langle kl \rangle$ represents the neighbouring pairs of k and l. $J$ is the coupling constant
	and represents the strength of spin interaction. We produced all the results in this paper with $J=1$. The second term is here to account for a potential external magnetic
	field with $h$ acting as the strength of this magnetic field. This paper will not consider an
	external magnetic field, so $h=0$ and eq. \ref{eq:hamilton} becomes

	\begin{equation}
		H(\textbf{s}) = -J \sum_{\langle kl \rangle} s_ks_l \label{eq:Hamiltonian}
	\end{equation}

	In statistical mechanics, we use probability and observables. In our problem, we have two
	observables the magnetisation and the energy, respectively

	\begin{align}
		M(\textbf{s}) &= \sum_{i=1} s_i \label{eq:M}\\
		E(\textbf{s}) &= H(\textbf{s}) = -J \sum_{\langle kl \rangle} s_ks_l \label{eq:E}
	\end{align}

	$M$ is the magnetisation summing all spins in the lattice. Since we will study these
	observable for different lattice sizes, we introduce the normalised by spin magnetisation and
	energy

	\begin{align}
		m(\textbf{s}) &= \frac{M(\textbf{s})}{N} \label{eq:m} \\
		\epsilon(\textbf{s}) &= \frac{E(\textbf{s})}{N}  \label{eq:epsilon}
	\end{align}


	Given a temperature $T$ to determine the probability of being in the configuration
	$\textbf{s}$ we will use the Boltzmann distribution with the partition function $Z$

	\begin{align}
		p(\textbf{s},T) &= \frac{1}{Z} e^{-\beta E(\textbf{s})} \label{eq:B_probability} \\
		Z &= \sum_{\text{\textbf{s}}} e^{-\beta E(\textbf{s})} \label{eq:partition_fun}
	\end{align}

	where $\beta = \frac{1}{kT}$, $k$ the Boltzmann constant. the summation for the partition
	function means that we iterate over every possible s\textbf{s}. These numbers scale as $2^N$
	showing with the analytical calculation of the partition for a big lattice becomes quickly
	impossible. \\

	In statistical mechanics, we can only experimentally calculate the averaged values of our
	observable. Meaning that for a discrete observable $A(X)$ with probability $p(X)$ we have
	\begin{equation}
		\langle A \rangle = \sum_{X}A(X)p(X) \label{eq:expectation value}
	\end{equation}

	We can then apply eq. \ref{eq:expectation value} to our observables, and we can define two
	values of interest, $C_v$ and $\chi$. The first is the heat capacity, and $\chi$ is the
	magnetic susceptibility.

	\begin{align}
		C_v &= \frac{\langle E^2 \rangle - \langle E \rangle^2}{NkT^2} \label{eq:heat_capacity} \\
		\chi &= \frac{\langle M^2 \rangle - \langle |M| \rangle^2}{NKT} \label{eq:susceptibility}
	\end{align}

	\subsection{2x2 analytical solution}\label{subsec:analytical solution}

	Since it gets quickly impossible to calculate the probability distribution due to the
	partition function. We will first solve the 2x2 case where we only have $2^4=16$ possible
	$\textbf{s}$.

	\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\# $s_i=1$ & E [J/k] & M[1/J] & degeneracy \\
		\hline
		\hline
		0 & -4 &  -4 &  1  \\
		1 & 0&   -2&   4  \\
		2 & 0 & 0&  4  \\
		2 & 4 & 0 &  2 \\
		3 & 0  & 2 & 4 \\
		4 & -4 & 4 & 1 \\
		\hline
	\end{tabular}
	\caption{ Summary of all possible energy E, magnetisation M and the degeneracy is the
	number of spin configurations with the same characteristic} \label{tab:summary2x2}
	\end{table}

	Table \ref{tab:summary2x2} shows all possibilities in the 2x2 case, and we see that summing all
	of the degeneracy gives us 16, assuring us that we covered every possibility. Now that
	we have found these values. We can proceed with the calculation of the partition function.

	\begin{equation}
		Z = 12 + 4\cosh(8\beta) \label{eq:Analytical_partition}
	\end{equation}


	With Table \ref{tab:summary2x2}, eq. \ref{eq:Analytical_partition} and \ref{eq:expectation value} we obtain the following relations

	\begin{align*} \label{eq:sol}
		\langle E \rangle &= \frac{-32\sinh(8\beta)}{Z}  \\
		\langle \epsilon \rangle &= \frac{\langle E \rangle}{4} = \frac{-8\sinh(8\beta)}{Z} \\
		\langle E^2 \rangle &= \frac{256 \cosh(8\beta)}{Z} \\
		\langle M \rangle &= \frac{8}{Z} (2 + e^{8\beta}) \\
		\langle M^2 \rangle &= \frac{32}{Z} (1 + e^{8\beta}) \\
		\langle C_v \rangle &= \frac{\langle E^2 \rangle - \langle E \rangle^2}{4} \\
		\langle \chi \rangle &=  \frac{\langle M^2 \rangle - \langle |M| \rangle^2}{4}
	\end{align*}


	\subsection{Phase transition}\label{subsec:phase transition}
	One of the most interesting physical properties of the 2D Ising model is the phase transition.
	It means that our lattice will suddenly magnetise for a specific critical temperature $T_c$. For an infinite-size model, Lars Onsager found in 1944 that the theoretical value of

	\begin{equation}
		T_c(L=\infty)=\frac{2}{\log(1 + \sqrt{2})} J/k \approx 2.269 J/k \label{Tc_theo}
	\end{equation}

	Another important aspect of phase transition is that different system exhibit similar
	behaviour around the critical temperature. This is known as the critical exponents. We have the finite size from chapter 13 in the lecture notes\cite{Morten15}.

	\begin{equation}
		\xi \propto |T - T_c(L=\infty)|^{-\nu} \label{eq:critical exponent}
	\end{equation}

	with critical exponent $\nu=1$, $\xi$ is the correlation length and the largest possible
	value is $\xi=L$, which we associate with the critical temperature of our system. \\

	With our finite critical temperature $T_c(L)$, we can estimate numerically the infinite
	critical temperature with the scaling relation

	\begin{equation}
		T_c(L) = aL^{-1} + T_c(L=\infty) \label{eq:scaling relation}
	\end{equation}

	$a$ is a constant. This allows us to perform a linear regression to find an approximate value
	of the slope (a) and the intersect point, $T_c(L=\infty)$.
	% ===========================================
	
	

	\section{Methods}\label{sec:methods}
	%
	\subsection{MCMC and Metropolis algorithm} \label{subsec:mcmc}
	We will use the Monte Carlo Markov Chain (MCMC) to compute our model. This method use
	statistic to study deterministic problem like the Ising model. We define a probability
	distribution function and sample states from it. After some iteration or cycle, we obtain
	an ensemble of average, which allows us to understand a system when an analytical
	solution does not exist. \\

	A Markov Chain is a stochastic process where each step possesses a probability
	independent of the history. This means that the only relevant information needed is the
	latest step to calculate the probability of transitioning, but all previous states are
	unnecessary. This removes many constraints where keeping track of history would
	raise big memory issues. Fortunately, this is not the case here. \\

	For the Ising model, we have defined the distribution probability.

	\begin{equation}
		p_{\rm \textbf{s}} = \frac{e^{-\beta E(\textbf{s})}}{Z} \label{eq:prob distribution}
	\end{equation}


	Now we will need two assumptions. The first one is ergodicity. This means that every state
	can be reached within a finite amount of time. This allows us to consider every state's possibilities
	as we assume that no state can prevent us from reaching equilibrium (the states
	with the biggest probability). We also use the detailed balance. This means that if we have
	two states $P_1$ and $P_2$, we have as much transition from state 1 to 2 then 2 to 1. Here
	this means that:

	\begin{align}
		P_1 T(1:2) &= P_2T(2:1) \\
		\frac{T(1:2)}{T(2:1)} &= \frac{P_2}{P_1} \\
		&= e^{-\beta \Delta E} \label{eq:deltaE}
	\end{align}

	Where $T(i:j)$ denotes the transition probability of going from state i to j. The last line
	is obtained when we replace the $P_i$ with eq \ref{eq:prob distribution}. \\

	However, we do not know the actual transition probability, but we do not need to know
	it with the Metropolis algorithm. This algorithm creates an acceptance function that respects
	our system's constraint and is used to represent our transition probability. We assume
	that our transition probability should depend on \ref{eq:deltaE}. This gives us

	\begin{equation}
		T(1:2) = \text{min}(1, e^{-\beta \Delta E\\}) \label{eq:metropolis}
	\end{equation}


	This equation satisfies all of our constraints. We can see that the transition can only
	take values between 0 and 1.

	% ===========================================
	\subsection*{The algorithm}\label{subsec:you suck eloi}
	%

	We create a lattice of size $L \times L $ full of spin of values \{-1, 1\}. Since we
	consider boundary conditions, we use the modulo operator to access the spin at the beginning
	or end of our lattice once we reach the opposite side while calculating the Hamiltonian. We
	initialise our lattice either randomly or ordered with all spin in the up position (value
	of 1). \\

	Once we have initialised our system with the correct temperature and a random initial
	state. We start our Monte Carlo algorithm. One cycle corresponds of N spin flip attempt.
	N being the total number of spins. We take one random spin, flip it, calculate the
	energy difference, and have three options. If $\Delta E \le 0 $, we accept the state,
	change our lattice and compute the magnetisation and the energy. If $\Delta E > 0$, we have
	two possibilities. We will draw a random number u between 0 and 1 from a uniform distribution
	, and if $u < \Delta E$, we accept the new state. If that is not the case, we refuse the state to keep our current state and add their energy and magnetisation to our data once more. \\

	We will compute $K$ number of cycles, but we will keep only $K - N_{\rm burn}$ values
	because in statistical mechanics, we need to reach equilibrium to match the experimental.
	This method is called the burn-in method and assumes that $N_{\rm burn}$ cycle
	is enough to reach equilibrium in our system. The determination of this value will be
	discussed in more detail in Section \ref{subsec: burn in estimation}. \\

	Finally, we will use parallelisation with OpenMP to accelerate our computational time. We
	will use it to simulate many temperatures while studying the
	phase transition. This will allow us to parallelise the temperature loop as each iteration
	is independent. This operation reduced our computing time up to $50\%$.

	% ===========================================
	\section{Results}\label{sec:results}
	%
	\subsection{2x2 numerical simulation}\label{subsec: 2x2 numerical}
	We used $T=1 J/k$ and simulated \{500, 1000, 2000, 3000, 4000, 5000, 6000,
	7000, 8000, 9000, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000\}
	cycles and $L=2$. We computed the analytical solution described in Section \ref{subsec:analytical solution}.
	Figures \ref{fig:chi4} and \ref{fig:cv4} shows the expected value of $\chi$ and $C_v$ for the different number of cycle done. The theoretical values are plotted in a dashed line. We did
	not represented $\langle m \rangle$ or $\langle E \rangle$ as they are a necessity to compute $\chi$ and $C_v$ so
	finding good results for the last two shows that we have evaluated correctly the other
	quantities.

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.55]{figures/chi_problem4.pdf}
		\caption{}
		\label{fig:chi4}
	\end{figure}

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.55]{figures/CV_problem4.pdf}
		\caption{}
		\label{fig:cv4}
	\end{figure}

	\subsection{Burn-in time estimation}\label{subsec: burn in estimation}

	For the Burn-in testing, we took the same number of cycles as Section \ref{subsec: 2x2 numerical},
	with $L=20$ instead of $L=4$. We used two different temperatures $T_1=1 J/k$ and $T_2=2.4 J/k$
	starting from ordered (all spin being 1) and random. We plotted the expected value of $m$ and $\epsilon$ according to the number of cycles. The results are presented in
	Figures \ref{fig:eps5} and \ref{fig:m5}.

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.55]{figures/epsilon_problem5.pdf}
		\caption{}
		\label{fig:eps5}
	\end{figure}
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.55]{figures/m_problem5}
		\caption{}
		\label{fig:m5}
	\end{figure}

	\subsection{Distribution probability of $\epsilon$}\label{subsec:dist probability}

	For the probability distribution of $\epsilon$, we took the data set used in the section
	\ref{subsec: burn in estimation}, so 100000 cycles computed for both temperatures
	without any burn-in time and starting from a random position. Figures \ref{fig:hist1} and \ref{fig:hist2} shows the histogram of our distribution for their temperature.
	We also computed the variance and found $\sigma\approx0.00011$ for $T=1 J/k$ and
	$\sigma\approx0.02023$ for $T=2.4 J/k$.

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.55]{figures/Histo_T1.pdf}
		\caption{}
		\label{fig:hist1}
	\end{figure}

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.55]{figures/Histo_T2.pdf}
		\caption{}
		\label{fig:hist2}
	\end{figure}

	\subsection{Phase Transition}\label{subsec:num phase transition}

	MOM! IT IS NOT JUST A PHASE!!!!!
	
	% ===========================================
	\section{Discussion}\label{sec:discussion}
	%
	\subsection{Analytical and Numerical for L=2}\label{subsec:dis 2x2}
	
	Figures \ref{fig:chi4} and \ref{fig:cv4} shows the expected value of the susceptibility
	and the heat capacity for different MCMC cycle. We see that we reach quickly the theoretical
	value in 2000 cycles. Increasing the number of cycle will provide better and better 
	approximation as expected. However we notice that we never are perfectly constant and adding
	more cycles does not gives us the assurance that we would reach the theoretical value. This 
	can be explained due to the randomness of our algorithms. Indeed, if we start in a low probability state we can spend some time equilibrating which would weight heavily on the 
	expected value. This two figures shows that the burn in method would improve significantly 
	the numerical value.
	
	
	\subsection{Burn-in}\label{subsec:dis burnin}
	
	We know take a look at the equilibration time. There are many ways of calculating the required
	time for our algorithm to reach equilibrium time as discussed in chapter 13.9 in the lecture notes \cite{Morten15}. We chose
	to implement the burn-in method. This method consist in letting our code run for a certain number
	of cycle $N_{\rm burn}$ and once we reach the $N_{\rm burn} + 1$ cycle we consider that we have reached equilibrium and we start saving the data of interest.    
	
	
	\subsection{Distribution of $\epsilon$}\label{subsec:dis epsilon}
	
	\subsection{Parallelization}\label{subsec: dis parallelization}
	
	\subsection{Phase transition and critical temperature}\label{subsec: dis phase transition}
	
	% ===========================================
	\section{Conclusion}\label{sec:conclusion}
	We have studied the 2D Ising model with the Monte Carlo Markov Chain algorithm 
	using the Metropolis algorithm to sample the spin configuration. We found different 
	distribution of $\epsilon$ for different temperature and found out that it plays 
	an important role in the distribution's shape. The main focus on this paper is the 
	phase transition. For different lattice size we iterated over four different lattice 
	size $L=$\{40, 60, 80, 100 \}, used the MCMC algorithm to compute the expected values 
	of $\chi$ and $C_v$. This allowed us to found a critical temperature of $T_c(L=\infty)$
	to be $ 2.29 \pm 0.04 J/k $ where the error is calculated via the fitting function 
	\textit{linregress} from the \textbf{scipy} library. The numerical value is in the range
	of the theoretical value $T_{c}^{\rm th} \approx 2.269 J/k$.
	
	
	\onecolumngrid
	
	%\bibliographystyle{apalike}
	%\printbibliography[heading=bibintoc, title={Refrences}]
	\bibliography{refs}
	
	
\end{document}
